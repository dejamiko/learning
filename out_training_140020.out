Epoch [1/200], Train Loss: 0.1544, Val Loss: 0.1235
Epoch [2/200], Train Loss: 0.1103, Val Loss: 0.1024
Epoch [3/200], Train Loss: 0.0970, Val Loss: 0.1040
Epoch [4/200], Train Loss: 0.0943, Val Loss: 0.1209
Epoch [5/200], Train Loss: 0.0876, Val Loss: 0.1069
Epoch [6/200], Train Loss: 0.0834, Val Loss: 0.1027
Epoch [7/200], Train Loss: 0.0774, Val Loss: 0.1025
Epoch [8/200], Train Loss: 0.0762, Val Loss: 0.1031
Epoch [9/200], Train Loss: 0.0763, Val Loss: 0.1034
Epoch [10/200], Train Loss: 0.0752, Val Loss: 0.1033
Epoch [11/200], Train Loss: 0.0737, Val Loss: 0.1032
Epoch [12/200], Train Loss: 0.0739, Val Loss: 0.1032
Epoch [13/200], Train Loss: 0.0726, Val Loss: 0.1030
Epoch [14/200], Train Loss: 0.0744, Val Loss: 0.1030
Epoch [15/200], Train Loss: 0.0730, Val Loss: 0.1036
Epoch [16/200], Train Loss: 0.0755, Val Loss: 0.1033
Epoch [17/200], Train Loss: 0.0723, Val Loss: 0.1032
Epoch [18/200], Train Loss: 0.0728, Val Loss: 0.1033
Epoch [19/200], Train Loss: 0.0720, Val Loss: 0.1033
Epoch [20/200], Train Loss: 0.0740, Val Loss: 0.1029
Epoch [21/200], Train Loss: 0.0724, Val Loss: 0.1034
Epoch [22/200], Train Loss: 0.0744, Val Loss: 0.1035
Early stopping
/vol/bitbucket/md1823/taskmaster/learning/optim/model_training.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load("checkpoint.pt"))
/vol/bitbucket/md1823/taskmaster/learning/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/vol/bitbucket/md1823/taskmaster/learning/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Epoch [1/200], Train Loss: 0.2139, Val Loss: 0.1654
Epoch [2/200], Train Loss: 0.1635, Val Loss: 0.1406
Epoch [3/200], Train Loss: 0.1545, Val Loss: 0.1539
Epoch [4/200], Train Loss: 0.1377, Val Loss: 0.1299
Epoch [5/200], Train Loss: 0.1321, Val Loss: 0.1275
Epoch [6/200], Train Loss: 0.1166, Val Loss: 0.1286
Epoch [7/200], Train Loss: 0.1119, Val Loss: 0.1218
Epoch [8/200], Train Loss: 0.1097, Val Loss: 0.1336
Epoch [9/200], Train Loss: 0.1130, Val Loss: 0.1219
Epoch [10/200], Train Loss: 0.1115, Val Loss: 0.1192
Epoch [11/200], Train Loss: 0.1088, Val Loss: 0.1204
Epoch [12/200], Train Loss: 0.1086, Val Loss: 0.1195
Epoch [13/200], Train Loss: 0.1088, Val Loss: 0.1201
Epoch [14/200], Train Loss: 0.1077, Val Loss: 0.1205
Epoch [15/200], Train Loss: 0.1076, Val Loss: 0.1200
Epoch [16/200], Train Loss: 0.1062, Val Loss: 0.1199
Epoch [17/200], Train Loss: 0.1080, Val Loss: 0.1194
Epoch [18/200], Train Loss: 0.1072, Val Loss: 0.1183
Epoch [19/200], Train Loss: 0.1070, Val Loss: 0.1191
Epoch [20/200], Train Loss: 0.1058, Val Loss: 0.1210
Epoch [21/200], Train Loss: 0.1063, Val Loss: 0.1184
Epoch [22/200], Train Loss: 0.1062, Val Loss: 0.1190
Epoch [23/200], Train Loss: 0.1063, Val Loss: 0.1204
Epoch [24/200], Train Loss: 0.1047, Val Loss: 0.1215
Epoch [25/200], Train Loss: 0.1108, Val Loss: 0.1186
Epoch [26/200], Train Loss: 0.1089, Val Loss: 0.1198
Epoch [27/200], Train Loss: 0.1072, Val Loss: 0.1196
Epoch [28/200], Train Loss: 0.1058, Val Loss: 0.1195
Epoch [29/200], Train Loss: 0.1085, Val Loss: 0.1206
Epoch [30/200], Train Loss: 0.1097, Val Loss: 0.1199
Epoch [31/200], Train Loss: 0.1051, Val Loss: 0.1208
Epoch [32/200], Train Loss: 0.1078, Val Loss: 0.1211
Epoch [33/200], Train Loss: 0.1081, Val Loss: 0.1194
Epoch [34/200], Train Loss: 0.1079, Val Loss: 0.1203
Epoch [35/200], Train Loss: 0.1023, Val Loss: 0.1205
Epoch [36/200], Train Loss: 0.1016, Val Loss: 0.1197
Epoch [37/200], Train Loss: 0.1102, Val Loss: 0.1193
Epoch [38/200], Train Loss: 0.1049, Val Loss: 0.1191
Early stopping
Epoch [1/200], Train Loss: 0.2421, Val Loss: 0.1960
Epoch [2/200], Train Loss: 0.2149, Val Loss: 0.2032
Epoch [3/200], Train Loss: 0.2075, Val Loss: 0.1913
Epoch [4/200], Train Loss: 0.1800, Val Loss: 0.2755
Epoch [5/200], Train Loss: 0.1780, Val Loss: 0.2011
Epoch [6/200], Train Loss: 0.1704, Val Loss: 0.1792
Epoch [7/200], Train Loss: 0.1665, Val Loss: 0.1769
Epoch [8/200], Train Loss: 0.1618, Val Loss: 0.1844
Epoch [9/200], Train Loss: 0.1625, Val Loss: 0.1883
Epoch [10/200], Train Loss: 0.1628, Val Loss: 0.1821
Epoch [11/200], Train Loss: 0.1560, Val Loss: 0.1850
Epoch [12/200], Train Loss: 0.1576, Val Loss: 0.1876
Epoch [13/200], Train Loss: 0.1566, Val Loss: 0.1885
Epoch [14/200], Train Loss: 0.1568, Val Loss: 0.1839
Epoch [15/200], Train Loss: 0.1570, Val Loss: 0.1842
Epoch [16/200], Train Loss: 0.1566, Val Loss: 0.1885
Epoch [17/200], Train Loss: 0.1553, Val Loss: 0.1877
Epoch [18/200], Train Loss: 0.1568, Val Loss: 0.1860
Epoch [19/200], Train Loss: 0.1549, Val Loss: 0.1841
Epoch [20/200], Train Loss: 0.1571, Val Loss: 0.1873
Epoch [21/200], Train Loss: 0.1541, Val Loss: 0.1898
Epoch [22/200], Train Loss: 0.1521, Val Loss: 0.1825
Epoch [23/200], Train Loss: 0.1599, Val Loss: 0.1892
Epoch [24/200], Train Loss: 0.1522, Val Loss: 0.1882
Epoch [25/200], Train Loss: 0.1556, Val Loss: 0.1828
Epoch [26/200], Train Loss: 0.1540, Val Loss: 0.1851
Epoch [27/200], Train Loss: 0.1570, Val Loss: 0.1868
Early stopping
Epoch [1/200], Train Loss: 0.1566, Val Loss: 0.1175
Epoch [2/200], Train Loss: 0.1095, Val Loss: 0.1485
Epoch [3/200], Train Loss: 0.0946, Val Loss: 0.1057
Epoch [4/200], Train Loss: 0.0894, Val Loss: 0.0994
Epoch [5/200], Train Loss: 0.0854, Val Loss: 0.1010
Epoch [6/200], Train Loss: 0.0786, Val Loss: 0.0999
Epoch [7/200], Train Loss: 0.0773, Val Loss: 0.0999
Epoch [8/200], Train Loss: 0.0748, Val Loss: 0.1001
Epoch [9/200], Train Loss: 0.0742, Val Loss: 0.1006
Epoch [10/200], Train Loss: 0.0723, Val Loss: 0.0990
Epoch [11/200], Train Loss: 0.0723, Val Loss: 0.0993
Epoch [12/200], Train Loss: 0.0728, Val Loss: 0.0996
Epoch [13/200], Train Loss: 0.0722, Val Loss: 0.0998
Epoch [14/200], Train Loss: 0.0715, Val Loss: 0.1000
Epoch [15/200], Train Loss: 0.0705, Val Loss: 0.0999
Epoch [16/200], Train Loss: 0.0709, Val Loss: 0.1000
Epoch [17/200], Train Loss: 0.0711, Val Loss: 0.1001
Epoch [18/200], Train Loss: 0.0712, Val Loss: 0.0999
Epoch [19/200], Train Loss: 0.0721, Val Loss: 0.1002
Epoch [20/200], Train Loss: 0.0731, Val Loss: 0.1001
Epoch [21/200], Train Loss: 0.0717, Val Loss: 0.0999
Epoch [22/200], Train Loss: 0.0701, Val Loss: 0.1000
Epoch [23/200], Train Loss: 0.0717, Val Loss: 0.1001
Epoch [24/200], Train Loss: 0.0709, Val Loss: 0.1000
Epoch [25/200], Train Loss: 0.0711, Val Loss: 0.0998
Epoch [26/200], Train Loss: 0.0712, Val Loss: 0.1000
Epoch [27/200], Train Loss: 0.0708, Val Loss: 0.0999
Epoch [28/200], Train Loss: 0.0725, Val Loss: 0.0999
Epoch [29/200], Train Loss: 0.0708, Val Loss: 0.1000
Epoch [30/200], Train Loss: 0.0720, Val Loss: 0.0999
Early stopping
/vol/bitbucket/md1823/taskmaster/learning/optim/model_training.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load("checkpoint.pt"))
/vol/bitbucket/md1823/taskmaster/learning/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/vol/bitbucket/md1823/taskmaster/learning/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Epoch [1/200], Train Loss: 0.1992, Val Loss: 0.1609
Epoch [2/200], Train Loss: 0.1559, Val Loss: 0.1412
Epoch [3/200], Train Loss: 0.1431, Val Loss: 0.2087
Epoch [4/200], Train Loss: 0.1429, Val Loss: 0.1337
Epoch [5/200], Train Loss: 0.1411, Val Loss: 0.1298
Epoch [6/200], Train Loss: 0.1146, Val Loss: 0.1263
Epoch [7/200], Train Loss: 0.1136, Val Loss: 0.1321
Epoch [8/200], Train Loss: 0.1109, Val Loss: 0.1211
Epoch [9/200], Train Loss: 0.1074, Val Loss: 0.1247
Epoch [10/200], Train Loss: 0.1076, Val Loss: 0.1222
Epoch [11/200], Train Loss: 0.1086, Val Loss: 0.1221
Epoch [12/200], Train Loss: 0.1081, Val Loss: 0.1217
Epoch [13/200], Train Loss: 0.1075, Val Loss: 0.1216
Epoch [14/200], Train Loss: 0.1030, Val Loss: 0.1227
Epoch [15/200], Train Loss: 0.1074, Val Loss: 0.1213
Epoch [16/200], Train Loss: 0.1085, Val Loss: 0.1211
Epoch [17/200], Train Loss: 0.1091, Val Loss: 0.1212
Epoch [18/200], Train Loss: 0.1060, Val Loss: 0.1213
Epoch [19/200], Train Loss: 0.1065, Val Loss: 0.1223
Epoch [20/200], Train Loss: 0.1055, Val Loss: 0.1210
Epoch [21/200], Train Loss: 0.1066, Val Loss: 0.1199
Epoch [22/200], Train Loss: 0.1089, Val Loss: 0.1208
Epoch [23/200], Train Loss: 0.1067, Val Loss: 0.1215
Epoch [24/200], Train Loss: 0.1033, Val Loss: 0.1210
Epoch [25/200], Train Loss: 0.1065, Val Loss: 0.1207
Epoch [26/200], Train Loss: 0.1049, Val Loss: 0.1221
Epoch [27/200], Train Loss: 0.1057, Val Loss: 0.1208
Epoch [28/200], Train Loss: 0.1054, Val Loss: 0.1234
Epoch [29/200], Train Loss: 0.1046, Val Loss: 0.1213
Epoch [30/200], Train Loss: 0.1080, Val Loss: 0.1225
Epoch [31/200], Train Loss: 0.1087, Val Loss: 0.1216
Epoch [32/200], Train Loss: 0.1060, Val Loss: 0.1220
Epoch [33/200], Train Loss: 0.1046, Val Loss: 0.1220
Epoch [34/200], Train Loss: 0.1075, Val Loss: 0.1216
Epoch [35/200], Train Loss: 0.1056, Val Loss: 0.1220
Epoch [36/200], Train Loss: 0.1064, Val Loss: 0.1225
Epoch [37/200], Train Loss: 0.1081, Val Loss: 0.1215
Epoch [38/200], Train Loss: 0.1092, Val Loss: 0.1198
Epoch [39/200], Train Loss: 0.1102, Val Loss: 0.1210
Epoch [40/200], Train Loss: 0.1083, Val Loss: 0.1219
Epoch [41/200], Train Loss: 0.1067, Val Loss: 0.1209
Early stopping
Epoch [1/200], Train Loss: 0.2528, Val Loss: 0.1845
Epoch [2/200], Train Loss: 0.2174, Val Loss: 0.2089
Epoch [3/200], Train Loss: 0.2066, Val Loss: 0.2310
Epoch [4/200], Train Loss: 0.2073, Val Loss: 0.2156
Epoch [5/200], Train Loss: 0.1957, Val Loss: 0.1699
Epoch [6/200], Train Loss: 0.1831, Val Loss: 0.1745
Epoch [7/200], Train Loss: 0.1774, Val Loss: 0.1749
Epoch [8/200], Train Loss: 0.1758, Val Loss: 0.1801
Epoch [9/200], Train Loss: 0.1737, Val Loss: 0.1834
Epoch [10/200], Train Loss: 0.1712, Val Loss: 0.1887
Epoch [11/200], Train Loss: 0.1718, Val Loss: 0.1833
Epoch [12/200], Train Loss: 0.1704, Val Loss: 0.1818
Epoch [13/200], Train Loss: 0.1746, Val Loss: 0.1809
Epoch [14/200], Train Loss: 0.1715, Val Loss: 0.1806
Epoch [15/200], Train Loss: 0.1711, Val Loss: 0.1815
Epoch [16/200], Train Loss: 0.1686, Val Loss: 0.1822
Epoch [17/200], Train Loss: 0.1719, Val Loss: 0.1834
Epoch [18/200], Train Loss: 0.1705, Val Loss: 0.1821
Epoch [19/200], Train Loss: 0.1711, Val Loss: 0.1809
Epoch [20/200], Train Loss: 0.1694, Val Loss: 0.1816
Epoch [21/200], Train Loss: 0.1737, Val Loss: 0.1814
Epoch [22/200], Train Loss: 0.1717, Val Loss: 0.1814
Epoch [23/200], Train Loss: 0.1726, Val Loss: 0.1826
Epoch [24/200], Train Loss: 0.1710, Val Loss: 0.1816
Epoch [25/200], Train Loss: 0.1740, Val Loss: 0.1808
Early stopping
Epoch [1/200], Train Loss: 0.1295, Val Loss: 0.1709
Epoch [2/200], Train Loss: 0.1046, Val Loss: 0.1414
Epoch [3/200], Train Loss: 0.0957, Val Loss: 0.1048
Epoch [4/200], Train Loss: 0.0907, Val Loss: 0.1092
Epoch [5/200], Train Loss: 0.0856, Val Loss: 0.1127
Epoch [6/200], Train Loss: 0.0816, Val Loss: 0.1048
Epoch [7/200], Train Loss: 0.0763, Val Loss: 0.1051
Epoch [8/200], Train Loss: 0.0751, Val Loss: 0.1042
Epoch [9/200], Train Loss: 0.0752, Val Loss: 0.1066
Epoch [10/200], Train Loss: 0.0740, Val Loss: 0.1044
Epoch [11/200], Train Loss: 0.0731, Val Loss: 0.1048
Epoch [12/200], Train Loss: 0.0735, Val Loss: 0.1049
Epoch [13/200], Train Loss: 0.0736, Val Loss: 0.1048
Epoch [14/200], Train Loss: 0.0723, Val Loss: 0.1047
Epoch [15/200], Train Loss: 0.0725, Val Loss: 0.1046
Epoch [16/200], Train Loss: 0.0721, Val Loss: 0.1045
Epoch [17/200], Train Loss: 0.0713, Val Loss: 0.1045
Epoch [18/200], Train Loss: 0.0714, Val Loss: 0.1049
Epoch [19/200], Train Loss: 0.0705, Val Loss: 0.1044
Epoch [20/200], Train Loss: 0.0726, Val Loss: 0.1043
Epoch [21/200], Train Loss: 0.0731, Val Loss: 0.1048
Epoch [22/200], Train Loss: 0.0719, Val Loss: 0.1047
Epoch [23/200], Train Loss: 0.0725, Val Loss: 0.1045
Epoch [24/200], Train Loss: 0.0731, Val Loss: 0.1047
Epoch [25/200], Train Loss: 0.0721, Val Loss: 0.1044
Epoch [26/200], Train Loss: 0.0711, Val Loss: 0.1045
Epoch [27/200], Train Loss: 0.0715, Val Loss: 0.1047
Epoch [28/200], Train Loss: 0.0716, Val Loss: 0.1045
Early stopping
/vol/bitbucket/md1823/taskmaster/learning/optim/model_training.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load("checkpoint.pt"))
/vol/bitbucket/md1823/taskmaster/learning/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/vol/bitbucket/md1823/taskmaster/learning/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Epoch [1/200], Train Loss: 0.2125, Val Loss: 0.1480
Epoch [2/200], Train Loss: 0.1640, Val Loss: 0.1343
Epoch [3/200], Train Loss: 0.1630, Val Loss: 0.1404
Epoch [4/200], Train Loss: 0.1359, Val Loss: 0.1371
Epoch [5/200], Train Loss: 0.1341, Val Loss: 0.1272
Epoch [6/200], Train Loss: 0.1188, Val Loss: 0.1219
Epoch [7/200], Train Loss: 0.1150, Val Loss: 0.1268
Epoch [8/200], Train Loss: 0.1165, Val Loss: 0.1238
Epoch [9/200], Train Loss: 0.1166, Val Loss: 0.1254
Epoch [10/200], Train Loss: 0.1072, Val Loss: 0.1142
Epoch [11/200], Train Loss: 0.1087, Val Loss: 0.1175
Epoch [12/200], Train Loss: 0.1085, Val Loss: 0.1188
Epoch [13/200], Train Loss: 0.1103, Val Loss: 0.1185
Epoch [14/200], Train Loss: 0.1083, Val Loss: 0.1194
Epoch [15/200], Train Loss: 0.1092, Val Loss: 0.1194
Epoch [16/200], Train Loss: 0.1073, Val Loss: 0.1185
Epoch [17/200], Train Loss: 0.1091, Val Loss: 0.1186
Epoch [18/200], Train Loss: 0.1087, Val Loss: 0.1180
Epoch [19/200], Train Loss: 0.1104, Val Loss: 0.1201
Epoch [20/200], Train Loss: 0.1113, Val Loss: 0.1202
Epoch [21/200], Train Loss: 0.1076, Val Loss: 0.1198
Epoch [22/200], Train Loss: 0.1081, Val Loss: 0.1190
Epoch [23/200], Train Loss: 0.1080, Val Loss: 0.1198
Epoch [24/200], Train Loss: 0.1099, Val Loss: 0.1192
Epoch [25/200], Train Loss: 0.1079, Val Loss: 0.1184
Epoch [26/200], Train Loss: 0.1062, Val Loss: 0.1197
Epoch [27/200], Train Loss: 0.1097, Val Loss: 0.1179
Epoch [28/200], Train Loss: 0.1086, Val Loss: 0.1201
Epoch [29/200], Train Loss: 0.1094, Val Loss: 0.1195
Epoch [30/200], Train Loss: 0.1095, Val Loss: 0.1167
Early stopping
Epoch [1/200], Train Loss: 0.2379, Val Loss: 0.2220
Epoch [2/200], Train Loss: 0.2066, Val Loss: 0.2100
Epoch [3/200], Train Loss: 0.2091, Val Loss: 0.1797
Epoch [4/200], Train Loss: 0.1795, Val Loss: 0.2033
Epoch [5/200], Train Loss: 0.1772, Val Loss: 0.1598
Epoch [6/200], Train Loss: 0.1660, Val Loss: 0.1521
Epoch [7/200], Train Loss: 0.1678, Val Loss: 0.1514
Epoch [8/200], Train Loss: 0.1707, Val Loss: 0.1509
Epoch [9/200], Train Loss: 0.1659, Val Loss: 0.1468
Epoch [10/200], Train Loss: 0.1675, Val Loss: 0.1503
Epoch [11/200], Train Loss: 0.1617, Val Loss: 0.1477
Epoch [12/200], Train Loss: 0.1633, Val Loss: 0.1463
Epoch [13/200], Train Loss: 0.1630, Val Loss: 0.1464
Epoch [14/200], Train Loss: 0.1633, Val Loss: 0.1457
Epoch [15/200], Train Loss: 0.1612, Val Loss: 0.1464
Epoch [16/200], Train Loss: 0.1627, Val Loss: 0.1466
Epoch [17/200], Train Loss: 0.1628, Val Loss: 0.1453
Epoch [18/200], Train Loss: 0.1610, Val Loss: 0.1454
Epoch [19/200], Train Loss: 0.1655, Val Loss: 0.1465
Epoch [20/200], Train Loss: 0.1641, Val Loss: 0.1475
Epoch [21/200], Train Loss: 0.1599, Val Loss: 0.1453
Epoch [22/200], Train Loss: 0.1606, Val Loss: 0.1456
Epoch [23/200], Train Loss: 0.1603, Val Loss: 0.1454
Epoch [24/200], Train Loss: 0.1605, Val Loss: 0.1454
Epoch [25/200], Train Loss: 0.1641, Val Loss: 0.1461
Epoch [26/200], Train Loss: 0.1619, Val Loss: 0.1463
Epoch [27/200], Train Loss: 0.1616, Val Loss: 0.1454
Epoch [28/200], Train Loss: 0.1622, Val Loss: 0.1452
Epoch [29/200], Train Loss: 0.1606, Val Loss: 0.1453
Epoch [30/200], Train Loss: 0.1608, Val Loss: 0.1453
Epoch [31/200], Train Loss: 0.1632, Val Loss: 0.1452
Epoch [32/200], Train Loss: 0.1634, Val Loss: 0.1454
Epoch [33/200], Train Loss: 0.1585, Val Loss: 0.1453
Epoch [34/200], Train Loss: 0.1591, Val Loss: 0.1454
Epoch [35/200], Train Loss: 0.1625, Val Loss: 0.1454
Epoch [36/200], Train Loss: 0.1607, Val Loss: 0.1462
Epoch [37/200], Train Loss: 0.1642, Val Loss: 0.1458
Early stopping
Epoch [1/200], Train Loss: 0.1340, Val Loss: 0.1283
Epoch [2/200], Train Loss: 0.1073, Val Loss: 0.1132
Epoch [3/200], Train Loss: 0.0961, Val Loss: 0.1153
Epoch [4/200], Train Loss: 0.0932, Val Loss: 0.1184
Epoch [5/200], Train Loss: 0.0859, Val Loss: 0.1140
Epoch [6/200], Train Loss: 0.0800, Val Loss: 0.1025
Epoch [7/200], Train Loss: 0.0759, Val Loss: 0.1021
Epoch [8/200], Train Loss: 0.0743, Val Loss: 0.1015
Epoch [9/200], Train Loss: 0.0730, Val Loss: 0.1012
Epoch [10/200], Train Loss: 0.0725, Val Loss: 0.1012
Epoch [11/200], Train Loss: 0.0732, Val Loss: 0.1019
Epoch [12/200], Train Loss: 0.0711, Val Loss: 0.1017
Epoch [13/200], Train Loss: 0.0714, Val Loss: 0.1019
Epoch [14/200], Train Loss: 0.0710, Val Loss: 0.1015
Epoch [15/200], Train Loss: 0.0709, Val Loss: 0.1024
Epoch [16/200], Train Loss: 0.0699, Val Loss: 0.1021
Epoch [17/200], Train Loss: 0.0722, Val Loss: 0.1024
Epoch [18/200], Train Loss: 0.0707, Val Loss: 0.1016
Epoch [19/200], Train Loss: 0.0709, Val Loss: 0.1021
Epoch [20/200], Train Loss: 0.0703, Val Loss: 0.1019
Epoch [21/200], Train Loss: 0.0705, Val Loss: 0.1022
Epoch [22/200], Train Loss: 0.0702, Val Loss: 0.1021
Epoch [23/200], Train Loss: 0.0706, Val Loss: 0.1022
Epoch [24/200], Train Loss: 0.0703, Val Loss: 0.1019
Epoch [25/200], Train Loss: 0.0701, Val Loss: 0.1019
Epoch [26/200], Train Loss: 0.0705, Val Loss: 0.1018
Epoch [27/200], Train Loss: 0.0709, Val Loss: 0.1019
Epoch [28/200], Train Loss: 0.0699, Val Loss: 0.1021
Epoch [29/200], Train Loss: 0.0715, Val Loss: 0.1017
Early stopping
/vol/bitbucket/md1823/taskmaster/learning/optim/model_training.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load("checkpoint.pt"))
/vol/bitbucket/md1823/taskmaster/learning/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/vol/bitbucket/md1823/taskmaster/learning/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Epoch [1/200], Train Loss: 0.2128, Val Loss: 0.1651
Epoch [2/200], Train Loss: 0.1733, Val Loss: 0.1994
Epoch [3/200], Train Loss: 0.1674, Val Loss: 0.1681
Epoch [4/200], Train Loss: 0.1500, Val Loss: 0.1479
Epoch [5/200], Train Loss: 0.1401, Val Loss: 0.1475
Epoch [6/200], Train Loss: 0.1263, Val Loss: 0.1317
Epoch [7/200], Train Loss: 0.1158, Val Loss: 0.1289
Epoch [8/200], Train Loss: 0.1211, Val Loss: 0.1342
Epoch [9/200], Train Loss: 0.1152, Val Loss: 0.1316
Epoch [10/200], Train Loss: 0.1163, Val Loss: 0.1306
Epoch [11/200], Train Loss: 0.1152, Val Loss: 0.1307
Epoch [12/200], Train Loss: 0.1167, Val Loss: 0.1301
Epoch [13/200], Train Loss: 0.1142, Val Loss: 0.1308
Epoch [14/200], Train Loss: 0.1118, Val Loss: 0.1290
Epoch [15/200], Train Loss: 0.1138, Val Loss: 0.1301
Epoch [16/200], Train Loss: 0.1123, Val Loss: 0.1300
Epoch [17/200], Train Loss: 0.1157, Val Loss: 0.1302
Epoch [18/200], Train Loss: 0.1189, Val Loss: 0.1304
Epoch [19/200], Train Loss: 0.1126, Val Loss: 0.1299
Epoch [20/200], Train Loss: 0.1111, Val Loss: 0.1300
Epoch [21/200], Train Loss: 0.1152, Val Loss: 0.1295
Epoch [22/200], Train Loss: 0.1112, Val Loss: 0.1303
Epoch [23/200], Train Loss: 0.1124, Val Loss: 0.1295
Epoch [24/200], Train Loss: 0.1123, Val Loss: 0.1296
Epoch [25/200], Train Loss: 0.1125, Val Loss: 0.1299
Epoch [26/200], Train Loss: 0.1136, Val Loss: 0.1304
Epoch [27/200], Train Loss: 0.1170, Val Loss: 0.1297
Early stopping
Epoch [1/200], Train Loss: 0.2277, Val Loss: 0.2287
Epoch [2/200], Train Loss: 0.2094, Val Loss: 0.2763
Epoch [3/200], Train Loss: 0.2029, Val Loss: 0.2213
Epoch [4/200], Train Loss: 0.1886, Val Loss: 0.2763
Epoch [5/200], Train Loss: 0.1834, Val Loss: 0.2763
Epoch [6/200], Train Loss: 0.1672, Val Loss: 0.1600
Epoch [7/200], Train Loss: 0.1561, Val Loss: 0.1564
Epoch [8/200], Train Loss: 0.1510, Val Loss: 0.1577
Epoch [9/200], Train Loss: 0.1463, Val Loss: 0.1552
Epoch [10/200], Train Loss: 0.1455, Val Loss: 0.1523
Epoch [11/200], Train Loss: 0.1399, Val Loss: 0.1504
Epoch [12/200], Train Loss: 0.1370, Val Loss: 0.1498
Epoch [13/200], Train Loss: 0.1379, Val Loss: 0.1499
Epoch [14/200], Train Loss: 0.1388, Val Loss: 0.1460
Epoch [15/200], Train Loss: 0.1391, Val Loss: 0.1513
Epoch [16/200], Train Loss: 0.1394, Val Loss: 0.1499
Epoch [17/200], Train Loss: 0.1385, Val Loss: 0.1518
Epoch [18/200], Train Loss: 0.1373, Val Loss: 0.1494
Epoch [19/200], Train Loss: 0.1369, Val Loss: 0.1512
Epoch [20/200], Train Loss: 0.1383, Val Loss: 0.1508
Epoch [21/200], Train Loss: 0.1392, Val Loss: 0.1511
Epoch [22/200], Train Loss: 0.1388, Val Loss: 0.1509
Epoch [23/200], Train Loss: 0.1385, Val Loss: 0.1507
Epoch [24/200], Train Loss: 0.1396, Val Loss: 0.1499
Epoch [25/200], Train Loss: 0.1399, Val Loss: 0.1510
Epoch [26/200], Train Loss: 0.1366, Val Loss: 0.1497
Epoch [27/200], Train Loss: 0.1414, Val Loss: 0.1510
Epoch [28/200], Train Loss: 0.1370, Val Loss: 0.1497
Epoch [29/200], Train Loss: 0.1397, Val Loss: 0.1495
Epoch [30/200], Train Loss: 0.1371, Val Loss: 0.1472
Epoch [31/200], Train Loss: 0.1395, Val Loss: 0.1494
Epoch [32/200], Train Loss: 0.1378, Val Loss: 0.1501
Epoch [33/200], Train Loss: 0.1378, Val Loss: 0.1490
Epoch [34/200], Train Loss: 0.1401, Val Loss: 0.1504
Early stopping
/vol/bitbucket/md1823/taskmaster/learning/venv/lib/python3.10/site-packages/segment_anything/build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state_dict = torch.load(f)
Epoch [1/200], Train Loss: 0.1460, Val Loss: 0.1062
Epoch [2/200], Train Loss: 0.1049, Val Loss: 0.1105
Epoch [3/200], Train Loss: 0.0961, Val Loss: 0.1127
Epoch [4/200], Train Loss: 0.0913, Val Loss: 0.1024
Epoch [5/200], Train Loss: 0.0883, Val Loss: 0.0987
Epoch [6/200], Train Loss: 0.0775, Val Loss: 0.0995
Epoch [7/200], Train Loss: 0.0768, Val Loss: 0.1007
Epoch [8/200], Train Loss: 0.0738, Val Loss: 0.1000
Epoch [9/200], Train Loss: 0.0740, Val Loss: 0.1016
Epoch [10/200], Train Loss: 0.0731, Val Loss: 0.1023
Epoch [11/200], Train Loss: 0.0728, Val Loss: 0.1021
Epoch [12/200], Train Loss: 0.0732, Val Loss: 0.1020
Epoch [13/200], Train Loss: 0.0719, Val Loss: 0.1019
Epoch [14/200], Train Loss: 0.0720, Val Loss: 0.1017
Epoch [15/200], Train Loss: 0.0730, Val Loss: 0.1019
Epoch [16/200], Train Loss: 0.0716, Val Loss: 0.1019
Epoch [17/200], Train Loss: 0.0721, Val Loss: 0.1021
Epoch [18/200], Train Loss: 0.0718, Val Loss: 0.1020
Epoch [19/200], Train Loss: 0.0719, Val Loss: 0.1018
Epoch [20/200], Train Loss: 0.0706, Val Loss: 0.1023
Epoch [21/200], Train Loss: 0.0717, Val Loss: 0.1023
Epoch [22/200], Train Loss: 0.0731, Val Loss: 0.1019
Epoch [23/200], Train Loss: 0.0712, Val Loss: 0.1023
Epoch [24/200], Train Loss: 0.0736, Val Loss: 0.1022
Epoch [25/200], Train Loss: 0.0720, Val Loss: 0.1020
Early stopping
/vol/bitbucket/md1823/taskmaster/learning/optim/model_training.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load("checkpoint.pt"))
/vol/bitbucket/md1823/taskmaster/learning/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/vol/bitbucket/md1823/taskmaster/learning/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Epoch [1/200], Train Loss: 0.2108, Val Loss: 0.1971
Epoch [2/200], Train Loss: 0.1661, Val Loss: 0.1704
Epoch [3/200], Train Loss: 0.1433, Val Loss: 0.1610
Epoch [4/200], Train Loss: 0.1312, Val Loss: 0.1641
Epoch [5/200], Train Loss: 0.1227, Val Loss: 0.1345
Epoch [6/200], Train Loss: 0.1167, Val Loss: 0.1280
Epoch [7/200], Train Loss: 0.1117, Val Loss: 0.1227
Epoch [8/200], Train Loss: 0.1102, Val Loss: 0.1298
Epoch [9/200], Train Loss: 0.1110, Val Loss: 0.1231
Epoch [10/200], Train Loss: 0.1062, Val Loss: 0.1194
Epoch [11/200], Train Loss: 0.1059, Val Loss: 0.1214
Epoch [12/200], Train Loss: 0.1032, Val Loss: 0.1216
Epoch [13/200], Train Loss: 0.1068, Val Loss: 0.1201
Epoch [14/200], Train Loss: 0.1079, Val Loss: 0.1197
Epoch [15/200], Train Loss: 0.1041, Val Loss: 0.1204
Epoch [16/200], Train Loss: 0.1039, Val Loss: 0.1200
Epoch [17/200], Train Loss: 0.1069, Val Loss: 0.1204
Epoch [18/200], Train Loss: 0.1071, Val Loss: 0.1213
Epoch [19/200], Train Loss: 0.1046, Val Loss: 0.1196
Epoch [20/200], Train Loss: 0.1069, Val Loss: 0.1214
Epoch [21/200], Train Loss: 0.1039, Val Loss: 0.1206
Epoch [22/200], Train Loss: 0.1077, Val Loss: 0.1207
Epoch [23/200], Train Loss: 0.1035, Val Loss: 0.1206
Epoch [24/200], Train Loss: 0.1060, Val Loss: 0.1210
Epoch [25/200], Train Loss: 0.1047, Val Loss: 0.1187
Epoch [26/200], Train Loss: 0.1072, Val Loss: 0.1193
Epoch [27/200], Train Loss: 0.1016, Val Loss: 0.1202
Epoch [28/200], Train Loss: 0.1024, Val Loss: 0.1215
Epoch [29/200], Train Loss: 0.1076, Val Loss: 0.1204
Epoch [30/200], Train Loss: 0.1056, Val Loss: 0.1200
Epoch [31/200], Train Loss: 0.1072, Val Loss: 0.1199
Epoch [32/200], Train Loss: 0.1054, Val Loss: 0.1195
Epoch [33/200], Train Loss: 0.1033, Val Loss: 0.1201
Epoch [34/200], Train Loss: 0.1012, Val Loss: 0.1192
Epoch [35/200], Train Loss: 0.1058, Val Loss: 0.1184
Epoch [36/200], Train Loss: 0.1072, Val Loss: 0.1210
Epoch [37/200], Train Loss: 0.1077, Val Loss: 0.1195
Epoch [38/200], Train Loss: 0.1045, Val Loss: 0.1214
Epoch [39/200], Train Loss: 0.1057, Val Loss: 0.1194
Epoch [40/200], Train Loss: 0.1060, Val Loss: 0.1191
Epoch [41/200], Train Loss: 0.1051, Val Loss: 0.1205
Epoch [42/200], Train Loss: 0.1074, Val Loss: 0.1206
Epoch [43/200], Train Loss: 0.1046, Val Loss: 0.1194
Epoch [44/200], Train Loss: 0.1055, Val Loss: 0.1193
Epoch [45/200], Train Loss: 0.1039, Val Loss: 0.1201
Epoch [46/200], Train Loss: 0.1057, Val Loss: 0.1205
Epoch [47/200], Train Loss: 0.1039, Val Loss: 0.1194
Epoch [48/200], Train Loss: 0.1033, Val Loss: 0.1188
Epoch [49/200], Train Loss: 0.1054, Val Loss: 0.1198
Epoch [50/200], Train Loss: 0.1089, Val Loss: 0.1202
Epoch [51/200], Train Loss: 0.1046, Val Loss: 0.1201
Epoch [52/200], Train Loss: 0.1065, Val Loss: 0.1214
Epoch [53/200], Train Loss: 0.1053, Val Loss: 0.1199
Epoch [54/200], Train Loss: 0.1071, Val Loss: 0.1202
Epoch [55/200], Train Loss: 0.1060, Val Loss: 0.1207
Early stopping
Epoch [1/200], Train Loss: 0.2528, Val Loss: 0.2268
Epoch [2/200], Train Loss: 0.2127, Val Loss: 0.2202
Epoch [3/200], Train Loss: 0.2117, Val Loss: 0.2763
Epoch [4/200], Train Loss: 0.2149, Val Loss: 0.2107
Epoch [5/200], Train Loss: 0.2100, Val Loss: 0.2151
Epoch [6/200], Train Loss: 0.2095, Val Loss: 0.2085
Epoch [7/200], Train Loss: 0.2051, Val Loss: 0.2076
Epoch [8/200], Train Loss: 0.2019, Val Loss: 0.2003
Epoch [9/200], Train Loss: 0.2010, Val Loss: 0.1932
Epoch [10/200], Train Loss: 0.1917, Val Loss: 0.1824
Epoch [11/200], Train Loss: 0.1800, Val Loss: 0.1750
Epoch [12/200], Train Loss: 0.1785, Val Loss: 0.1725
Epoch [13/200], Train Loss: 0.1800, Val Loss: 0.1725
Epoch [14/200], Train Loss: 0.1753, Val Loss: 0.1706
Epoch [15/200], Train Loss: 0.1740, Val Loss: 0.1688
Epoch [16/200], Train Loss: 0.1763, Val Loss: 0.1687
Epoch [17/200], Train Loss: 0.1738, Val Loss: 0.1674
Epoch [18/200], Train Loss: 0.1750, Val Loss: 0.1671
Epoch [19/200], Train Loss: 0.1746, Val Loss: 0.1675
Epoch [20/200], Train Loss: 0.1782, Val Loss: 0.1677
Epoch [21/200], Train Loss: 0.1745, Val Loss: 0.1676
Epoch [22/200], Train Loss: 0.1743, Val Loss: 0.1673
Epoch [23/200], Train Loss: 0.1709, Val Loss: 0.1676
Epoch [24/200], Train Loss: 0.1734, Val Loss: 0.1673
Epoch [25/200], Train Loss: 0.1759, Val Loss: 0.1682
Epoch [26/200], Train Loss: 0.1740, Val Loss: 0.1676
Epoch [27/200], Train Loss: 0.1722, Val Loss: 0.1672
Epoch [28/200], Train Loss: 0.1728, Val Loss: 0.1668
Epoch [29/200], Train Loss: 0.1735, Val Loss: 0.1669
Epoch [30/200], Train Loss: 0.1739, Val Loss: 0.1680
Epoch [31/200], Train Loss: 0.1726, Val Loss: 0.1678
Epoch [32/200], Train Loss: 0.1741, Val Loss: 0.1667
Epoch [33/200], Train Loss: 0.1762, Val Loss: 0.1675
Epoch [34/200], Train Loss: 0.1731, Val Loss: 0.1689
Epoch [35/200], Train Loss: 0.1736, Val Loss: 0.1675
Epoch [36/200], Train Loss: 0.1738, Val Loss: 0.1678
Epoch [37/200], Train Loss: 0.1768, Val Loss: 0.1668
Epoch [38/200], Train Loss: 0.1776, Val Loss: 0.1682
Epoch [39/200], Train Loss: 0.1760, Val Loss: 0.1667
Epoch [40/200], Train Loss: 0.1751, Val Loss: 0.1675
Epoch [41/200], Train Loss: 0.1746, Val Loss: 0.1674
Epoch [42/200], Train Loss: 0.1766, Val Loss: 0.1677
Epoch [43/200], Train Loss: 0.1751, Val Loss: 0.1681
Epoch [44/200], Train Loss: 0.1738, Val Loss: 0.1691
Epoch [45/200], Train Loss: 0.1745, Val Loss: 0.1684
Epoch [46/200], Train Loss: 0.1776, Val Loss: 0.1675
Epoch [47/200], Train Loss: 0.1730, Val Loss: 0.1674
Epoch [48/200], Train Loss: 0.1720, Val Loss: 0.1683
Epoch [49/200], Train Loss: 0.1745, Val Loss: 0.1688
Epoch [50/200], Train Loss: 0.1749, Val Loss: 0.1670
Epoch [51/200], Train Loss: 0.1747, Val Loss: 0.1682
Epoch [52/200], Train Loss: 0.1773, Val Loss: 0.1683
Epoch [53/200], Train Loss: 0.1762, Val Loss: 0.1681
Epoch [54/200], Train Loss: 0.1741, Val Loss: 0.1693
Epoch [55/200], Train Loss: 0.1739, Val Loss: 0.1668
Epoch [56/200], Train Loss: 0.1728, Val Loss: 0.1677
Epoch [57/200], Train Loss: 0.1729, Val Loss: 0.1683
Epoch [58/200], Train Loss: 0.1773, Val Loss: 0.1687
Epoch [59/200], Train Loss: 0.1782, Val Loss: 0.1690
Early stopping
Epoch [1/200], Train Loss: 0.1413, Val Loss: 0.1083
Epoch [2/200], Train Loss: 0.1079, Val Loss: 0.1163
Epoch [3/200], Train Loss: 0.0995, Val Loss: 0.1017
Epoch [4/200], Train Loss: 0.0910, Val Loss: 0.1018
Epoch [5/200], Train Loss: 0.0891, Val Loss: 0.1017
Epoch [6/200], Train Loss: 0.0800, Val Loss: 0.1031
Epoch [7/200], Train Loss: 0.0771, Val Loss: 0.1002
Epoch [8/200], Train Loss: 0.0754, Val Loss: 0.1026
Epoch [9/200], Train Loss: 0.0750, Val Loss: 0.1031
Epoch [10/200], Train Loss: 0.0747, Val Loss: 0.1008
Epoch [11/200], Train Loss: 0.0739, Val Loss: 0.1011
Epoch [12/200], Train Loss: 0.0737, Val Loss: 0.1015
Epoch [13/200], Train Loss: 0.0732, Val Loss: 0.1014
Epoch [14/200], Train Loss: 0.0747, Val Loss: 0.1021
Epoch [15/200], Train Loss: 0.0734, Val Loss: 0.1018
Epoch [16/200], Train Loss: 0.0734, Val Loss: 0.1015
Epoch [17/200], Train Loss: 0.0731, Val Loss: 0.1015
Epoch [18/200], Train Loss: 0.0731, Val Loss: 0.1016
Epoch [19/200], Train Loss: 0.0740, Val Loss: 0.1017
Epoch [20/200], Train Loss: 0.0729, Val Loss: 0.1013
Epoch [21/200], Train Loss: 0.0743, Val Loss: 0.1016
Epoch [22/200], Train Loss: 0.0740, Val Loss: 0.1011
Epoch [23/200], Train Loss: 0.0735, Val Loss: 0.1013
Epoch [24/200], Train Loss: 0.0731, Val Loss: 0.1015
Epoch [25/200], Train Loss: 0.0716, Val Loss: 0.1017
Epoch [26/200], Train Loss: 0.0738, Val Loss: 0.1016
Epoch [27/200], Train Loss: 0.0732, Val Loss: 0.1012
Early stopping
/vol/bitbucket/md1823/taskmaster/learning/optim/model_training.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load("checkpoint.pt"))
/vol/bitbucket/md1823/taskmaster/learning/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/vol/bitbucket/md1823/taskmaster/learning/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Epoch [1/200], Train Loss: 0.2566, Val Loss: 0.1821
Epoch [2/200], Train Loss: 0.1840, Val Loss: 0.1829
Epoch [3/200], Train Loss: 0.1623, Val Loss: 0.1733
Epoch [4/200], Train Loss: 0.1445, Val Loss: 0.1345
Epoch [5/200], Train Loss: 0.1355, Val Loss: 0.1355
Epoch [6/200], Train Loss: 0.1226, Val Loss: 0.1213
Epoch [7/200], Train Loss: 0.1200, Val Loss: 0.1187
Epoch [8/200], Train Loss: 0.1170, Val Loss: 0.1164
Epoch [9/200], Train Loss: 0.1171, Val Loss: 0.1180
Epoch [10/200], Train Loss: 0.1139, Val Loss: 0.1176
Epoch [11/200], Train Loss: 0.1120, Val Loss: 0.1169
Epoch [12/200], Train Loss: 0.1120, Val Loss: 0.1164
Epoch [13/200], Train Loss: 0.1090, Val Loss: 0.1173
Epoch [14/200], Train Loss: 0.1143, Val Loss: 0.1166
Epoch [15/200], Train Loss: 0.1108, Val Loss: 0.1154
Epoch [16/200], Train Loss: 0.1116, Val Loss: 0.1158
Epoch [17/200], Train Loss: 0.1079, Val Loss: 0.1158
Epoch [18/200], Train Loss: 0.1103, Val Loss: 0.1155
Epoch [19/200], Train Loss: 0.1094, Val Loss: 0.1159
Epoch [20/200], Train Loss: 0.1088, Val Loss: 0.1156
Epoch [21/200], Train Loss: 0.1111, Val Loss: 0.1162
Epoch [22/200], Train Loss: 0.1117, Val Loss: 0.1166
Epoch [23/200], Train Loss: 0.1122, Val Loss: 0.1156
Epoch [24/200], Train Loss: 0.1121, Val Loss: 0.1164
Epoch [25/200], Train Loss: 0.1120, Val Loss: 0.1157
Epoch [26/200], Train Loss: 0.1066, Val Loss: 0.1163
Epoch [27/200], Train Loss: 0.1109, Val Loss: 0.1167
Epoch [28/200], Train Loss: 0.1125, Val Loss: 0.1158
Epoch [29/200], Train Loss: 0.1095, Val Loss: 0.1149
Epoch [30/200], Train Loss: 0.1119, Val Loss: 0.1156
Epoch [31/200], Train Loss: 0.1099, Val Loss: 0.1167
Epoch [32/200], Train Loss: 0.1100, Val Loss: 0.1173
Epoch [33/200], Train Loss: 0.1099, Val Loss: 0.1158
Epoch [34/200], Train Loss: 0.1126, Val Loss: 0.1157
Epoch [35/200], Train Loss: 0.1097, Val Loss: 0.1156
Epoch [36/200], Train Loss: 0.1104, Val Loss: 0.1158
Epoch [37/200], Train Loss: 0.1104, Val Loss: 0.1169
Epoch [38/200], Train Loss: 0.1078, Val Loss: 0.1157
Epoch [39/200], Train Loss: 0.1136, Val Loss: 0.1164
Epoch [40/200], Train Loss: 0.1137, Val Loss: 0.1170
Epoch [41/200], Train Loss: 0.1082, Val Loss: 0.1154
Epoch [42/200], Train Loss: 0.1127, Val Loss: 0.1157
Epoch [43/200], Train Loss: 0.1136, Val Loss: 0.1159
Epoch [44/200], Train Loss: 0.1112, Val Loss: 0.1158
Epoch [45/200], Train Loss: 0.1149, Val Loss: 0.1170
Epoch [46/200], Train Loss: 0.1147, Val Loss: 0.1170
Epoch [47/200], Train Loss: 0.1148, Val Loss: 0.1153
Epoch [48/200], Train Loss: 0.1105, Val Loss: 0.1156
Epoch [49/200], Train Loss: 0.1116, Val Loss: 0.1157
Early stopping
Epoch [1/200], Train Loss: 0.2503, Val Loss: 0.2763
Epoch [2/200], Train Loss: 0.1965, Val Loss: 0.2021
Epoch [3/200], Train Loss: 0.1921, Val Loss: 0.1811
Epoch [4/200], Train Loss: 0.1896, Val Loss: 0.1643
Epoch [5/200], Train Loss: 0.1744, Val Loss: 0.1796
Epoch [6/200], Train Loss: 0.1599, Val Loss: 0.1469
Epoch [7/200], Train Loss: 0.1560, Val Loss: 0.1475
Epoch [8/200], Train Loss: 0.1522, Val Loss: 0.1456
Epoch [9/200], Train Loss: 0.1502, Val Loss: 0.1458
Epoch [10/200], Train Loss: 0.1522, Val Loss: 0.1419
Epoch [11/200], Train Loss: 0.1446, Val Loss: 0.1515
Epoch [12/200], Train Loss: 0.1494, Val Loss: 0.1421
Epoch [13/200], Train Loss: 0.1444, Val Loss: 0.1436
Epoch [14/200], Train Loss: 0.1505, Val Loss: 0.1419
Epoch [15/200], Train Loss: 0.1459, Val Loss: 0.1431
Epoch [16/200], Train Loss: 0.1484, Val Loss: 0.1396
Epoch [17/200], Train Loss: 0.1464, Val Loss: 0.1434
Epoch [18/200], Train Loss: 0.1465, Val Loss: 0.1392
Epoch [19/200], Train Loss: 0.1463, Val Loss: 0.1440
Epoch [20/200], Train Loss: 0.1470, Val Loss: 0.1386
Epoch [21/200], Train Loss: 0.1465, Val Loss: 0.1382
Epoch [22/200], Train Loss: 0.1492, Val Loss: 0.1413
Epoch [23/200], Train Loss: 0.1484, Val Loss: 0.1392
Epoch [24/200], Train Loss: 0.1483, Val Loss: 0.1388
Epoch [25/200], Train Loss: 0.1463, Val Loss: 0.1392
Epoch [26/200], Train Loss: 0.1466, Val Loss: 0.1391
Epoch [27/200], Train Loss: 0.1462, Val Loss: 0.1390
Epoch [28/200], Train Loss: 0.1455, Val Loss: 0.1393
Epoch [29/200], Train Loss: 0.1445, Val Loss: 0.1386
Epoch [30/200], Train Loss: 0.1451, Val Loss: 0.1397
Epoch [31/200], Train Loss: 0.1449, Val Loss: 0.1428
Epoch [32/200], Train Loss: 0.1466, Val Loss: 0.1387
Epoch [33/200], Train Loss: 0.1472, Val Loss: 0.1394
Epoch [34/200], Train Loss: 0.1461, Val Loss: 0.1401
Epoch [35/200], Train Loss: 0.1484, Val Loss: 0.1386
Epoch [36/200], Train Loss: 0.1476, Val Loss: 0.1409
Epoch [37/200], Train Loss: 0.1458, Val Loss: 0.1390
Epoch [38/200], Train Loss: 0.1447, Val Loss: 0.1399
Epoch [39/200], Train Loss: 0.1482, Val Loss: 0.1384
Epoch [40/200], Train Loss: 0.1437, Val Loss: 0.1402
Epoch [41/200], Train Loss: 0.1469, Val Loss: 0.1387
Early stopping
Epoch [1/200], Train Loss: 0.1376, Val Loss: 0.1137
Epoch [2/200], Train Loss: 0.1076, Val Loss: 0.0969
Epoch [3/200], Train Loss: 0.0973, Val Loss: 0.1138
Epoch [4/200], Train Loss: 0.0912, Val Loss: 0.1012
Epoch [5/200], Train Loss: 0.0867, Val Loss: 0.0961
Epoch [6/200], Train Loss: 0.0778, Val Loss: 0.1009
Epoch [7/200], Train Loss: 0.0750, Val Loss: 0.1032
Epoch [8/200], Train Loss: 0.0752, Val Loss: 0.1021
Epoch [9/200], Train Loss: 0.0744, Val Loss: 0.1005
Epoch [10/200], Train Loss: 0.0734, Val Loss: 0.1028
Epoch [11/200], Train Loss: 0.0722, Val Loss: 0.1007
Epoch [12/200], Train Loss: 0.0729, Val Loss: 0.0997
Epoch [13/200], Train Loss: 0.0723, Val Loss: 0.0995
Epoch [14/200], Train Loss: 0.0723, Val Loss: 0.0994
Epoch [15/200], Train Loss: 0.0726, Val Loss: 0.0995
Epoch [16/200], Train Loss: 0.0717, Val Loss: 0.0996
Epoch [17/200], Train Loss: 0.0713, Val Loss: 0.0999
Epoch [18/200], Train Loss: 0.0716, Val Loss: 0.0993
Epoch [19/200], Train Loss: 0.0710, Val Loss: 0.0993
Epoch [20/200], Train Loss: 0.0710, Val Loss: 0.0997
Epoch [21/200], Train Loss: 0.0699, Val Loss: 0.0995
Epoch [22/200], Train Loss: 0.0704, Val Loss: 0.0995
Epoch [23/200], Train Loss: 0.0718, Val Loss: 0.0996
Epoch [24/200], Train Loss: 0.0719, Val Loss: 0.0997
Epoch [25/200], Train Loss: 0.0716, Val Loss: 0.0998
Early stopping
/vol/bitbucket/md1823/taskmaster/learning/optim/model_training.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load("checkpoint.pt"))
/vol/bitbucket/md1823/taskmaster/learning/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/vol/bitbucket/md1823/taskmaster/learning/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Epoch [1/200], Train Loss: 0.2261, Val Loss: 0.1744
Epoch [2/200], Train Loss: 0.1556, Val Loss: 0.1611
Epoch [3/200], Train Loss: 0.1396, Val Loss: 0.1609
Epoch [4/200], Train Loss: 0.1387, Val Loss: 0.1378
Epoch [5/200], Train Loss: 0.1283, Val Loss: 0.1436
Epoch [6/200], Train Loss: 0.1189, Val Loss: 0.1314
Epoch [7/200], Train Loss: 0.1122, Val Loss: 0.1300
Epoch [8/200], Train Loss: 0.1135, Val Loss: 0.1377
Epoch [9/200], Train Loss: 0.1104, Val Loss: 0.1302
Epoch [10/200], Train Loss: 0.1102, Val Loss: 0.1307
Epoch [11/200], Train Loss: 0.1086, Val Loss: 0.1313
Epoch [12/200], Train Loss: 0.1055, Val Loss: 0.1288
Epoch [13/200], Train Loss: 0.1099, Val Loss: 0.1322
Epoch [14/200], Train Loss: 0.1051, Val Loss: 0.1301
Epoch [15/200], Train Loss: 0.1068, Val Loss: 0.1314
Epoch [16/200], Train Loss: 0.1072, Val Loss: 0.1301
Epoch [17/200], Train Loss: 0.1056, Val Loss: 0.1289
Epoch [18/200], Train Loss: 0.1081, Val Loss: 0.1307
Epoch [19/200], Train Loss: 0.1057, Val Loss: 0.1303
Epoch [20/200], Train Loss: 0.1069, Val Loss: 0.1311
Epoch [21/200], Train Loss: 0.1035, Val Loss: 0.1302
Epoch [22/200], Train Loss: 0.1071, Val Loss: 0.1315
Epoch [23/200], Train Loss: 0.1077, Val Loss: 0.1313
Epoch [24/200], Train Loss: 0.1090, Val Loss: 0.1323
Epoch [25/200], Train Loss: 0.1067, Val Loss: 0.1306
Epoch [26/200], Train Loss: 0.1074, Val Loss: 0.1301
Epoch [27/200], Train Loss: 0.1045, Val Loss: 0.1309
Epoch [28/200], Train Loss: 0.1041, Val Loss: 0.1318
Epoch [29/200], Train Loss: 0.1070, Val Loss: 0.1306
Epoch [30/200], Train Loss: 0.1064, Val Loss: 0.1306
Epoch [31/200], Train Loss: 0.1054, Val Loss: 0.1309
Epoch [32/200], Train Loss: 0.1077, Val Loss: 0.1311
Early stopping
Epoch [1/200], Train Loss: 0.2337, Val Loss: 0.2095
Epoch [2/200], Train Loss: 0.2106, Val Loss: 0.2050
Epoch [3/200], Train Loss: 0.2091, Val Loss: 0.2059
Epoch [4/200], Train Loss: 0.1912, Val Loss: 0.1982
Epoch [5/200], Train Loss: 0.1883, Val Loss: 0.1943
Epoch [6/200], Train Loss: 0.1743, Val Loss: 0.1742
Epoch [7/200], Train Loss: 0.1698, Val Loss: 0.1644
Epoch [8/200], Train Loss: 0.1707, Val Loss: 0.1745
Epoch [9/200], Train Loss: 0.1667, Val Loss: 0.1715
Epoch [10/200], Train Loss: 0.1667, Val Loss: 0.1690
Epoch [11/200], Train Loss: 0.1636, Val Loss: 0.1689
Epoch [12/200], Train Loss: 0.1638, Val Loss: 0.1669
Epoch [13/200], Train Loss: 0.1638, Val Loss: 0.1658
Epoch [14/200], Train Loss: 0.1640, Val Loss: 0.1653
Epoch [15/200], Train Loss: 0.1654, Val Loss: 0.1658
Epoch [16/200], Train Loss: 0.1629, Val Loss: 0.1654
Epoch [17/200], Train Loss: 0.1614, Val Loss: 0.1678
Epoch [18/200], Train Loss: 0.1627, Val Loss: 0.1667
Epoch [19/200], Train Loss: 0.1603, Val Loss: 0.1658
Epoch [20/200], Train Loss: 0.1640, Val Loss: 0.1672
Epoch [21/200], Train Loss: 0.1598, Val Loss: 0.1661
Epoch [22/200], Train Loss: 0.1630, Val Loss: 0.1668
Epoch [23/200], Train Loss: 0.1612, Val Loss: 0.1662
Epoch [24/200], Train Loss: 0.1619, Val Loss: 0.1652
Epoch [25/200], Train Loss: 0.1608, Val Loss: 0.1663
Epoch [26/200], Train Loss: 0.1641, Val Loss: 0.1672
Epoch [27/200], Train Loss: 0.1584, Val Loss: 0.1662
Early stopping
Epoch [1/200], Train Loss: 0.1323, Val Loss: 0.1432
Epoch [2/200], Train Loss: 0.1023, Val Loss: 0.1055
Epoch [3/200], Train Loss: 0.0989, Val Loss: 0.1052
Epoch [4/200], Train Loss: 0.0883, Val Loss: 0.1028
Epoch [5/200], Train Loss: 0.0850, Val Loss: 0.1055
Epoch [6/200], Train Loss: 0.0799, Val Loss: 0.1039
Epoch [7/200], Train Loss: 0.0771, Val Loss: 0.1022
Epoch [8/200], Train Loss: 0.0736, Val Loss: 0.1032
Epoch [9/200], Train Loss: 0.0712, Val Loss: 0.1030
Epoch [10/200], Train Loss: 0.0714, Val Loss: 0.1015
Epoch [11/200], Train Loss: 0.0714, Val Loss: 0.1015
Epoch [12/200], Train Loss: 0.0691, Val Loss: 0.1016
Epoch [13/200], Train Loss: 0.0720, Val Loss: 0.1020
Epoch [14/200], Train Loss: 0.0702, Val Loss: 0.1018
Epoch [15/200], Train Loss: 0.0709, Val Loss: 0.1019
Epoch [16/200], Train Loss: 0.0703, Val Loss: 0.1020
Epoch [17/200], Train Loss: 0.0697, Val Loss: 0.1020
Epoch [18/200], Train Loss: 0.0697, Val Loss: 0.1019
Epoch [19/200], Train Loss: 0.0706, Val Loss: 0.1019
Epoch [20/200], Train Loss: 0.0695, Val Loss: 0.1019
Epoch [21/200], Train Loss: 0.0696, Val Loss: 0.1021
Epoch [22/200], Train Loss: 0.0698, Val Loss: 0.1019
Epoch [23/200], Train Loss: 0.0695, Val Loss: 0.1019
Epoch [24/200], Train Loss: 0.0700, Val Loss: 0.1019
Epoch [25/200], Train Loss: 0.0699, Val Loss: 0.1020
Epoch [26/200], Train Loss: 0.0708, Val Loss: 0.1019
Epoch [27/200], Train Loss: 0.0704, Val Loss: 0.1019
Epoch [28/200], Train Loss: 0.0717, Val Loss: 0.1020
Epoch [29/200], Train Loss: 0.0709, Val Loss: 0.1019
Epoch [30/200], Train Loss: 0.0703, Val Loss: 0.1019
Early stopping
/vol/bitbucket/md1823/taskmaster/learning/optim/model_training.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load("checkpoint.pt"))
/vol/bitbucket/md1823/taskmaster/learning/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/vol/bitbucket/md1823/taskmaster/learning/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Epoch [1/200], Train Loss: 0.2180, Val Loss: 0.1929
Epoch [2/200], Train Loss: 0.1725, Val Loss: 0.1715
Epoch [3/200], Train Loss: 0.1464, Val Loss: 0.1415
Epoch [4/200], Train Loss: 0.1432, Val Loss: 0.1583
Epoch [5/200], Train Loss: 0.1398, Val Loss: 0.1349
Epoch [6/200], Train Loss: 0.1206, Val Loss: 0.1320
Epoch [7/200], Train Loss: 0.1139, Val Loss: 0.1261
Epoch [8/200], Train Loss: 0.1214, Val Loss: 0.1219
Epoch [9/200], Train Loss: 0.1156, Val Loss: 0.1269
Epoch [10/200], Train Loss: 0.1135, Val Loss: 0.1304
Epoch [11/200], Train Loss: 0.1159, Val Loss: 0.1247
Epoch [12/200], Train Loss: 0.1117, Val Loss: 0.1249
Epoch [13/200], Train Loss: 0.1122, Val Loss: 0.1249
Epoch [14/200], Train Loss: 0.1152, Val Loss: 0.1249
Epoch [15/200], Train Loss: 0.1128, Val Loss: 0.1227
Epoch [16/200], Train Loss: 0.1141, Val Loss: 0.1250
Epoch [17/200], Train Loss: 0.1142, Val Loss: 0.1222
Epoch [18/200], Train Loss: 0.1138, Val Loss: 0.1222
Epoch [19/200], Train Loss: 0.1102, Val Loss: 0.1235
Epoch [20/200], Train Loss: 0.1120, Val Loss: 0.1234
Epoch [21/200], Train Loss: 0.1121, Val Loss: 0.1264
Epoch [22/200], Train Loss: 0.1122, Val Loss: 0.1245
Epoch [23/200], Train Loss: 0.1122, Val Loss: 0.1243
Epoch [24/200], Train Loss: 0.1153, Val Loss: 0.1265
Epoch [25/200], Train Loss: 0.1113, Val Loss: 0.1236
Epoch [26/200], Train Loss: 0.1100, Val Loss: 0.1257
Epoch [27/200], Train Loss: 0.1110, Val Loss: 0.1248
Epoch [28/200], Train Loss: 0.1129, Val Loss: 0.1232
Early stopping
Epoch [1/200], Train Loss: 0.2293, Val Loss: 0.2259
Epoch [2/200], Train Loss: 0.2106, Val Loss: 0.1963
Epoch [3/200], Train Loss: 0.2066, Val Loss: 0.2205
Epoch [4/200], Train Loss: 0.2074, Val Loss: 0.2036
Epoch [5/200], Train Loss: 0.1992, Val Loss: 0.2026
Epoch [6/200], Train Loss: 0.1775, Val Loss: 0.1701
Epoch [7/200], Train Loss: 0.1667, Val Loss: 0.1647
Epoch [8/200], Train Loss: 0.1684, Val Loss: 0.1750
Epoch [9/200], Train Loss: 0.1695, Val Loss: 0.1659
Epoch [10/200], Train Loss: 0.1674, Val Loss: 0.1706
Epoch [11/200], Train Loss: 0.1636, Val Loss: 0.1667
Epoch [12/200], Train Loss: 0.1626, Val Loss: 0.1651
Epoch [13/200], Train Loss: 0.1644, Val Loss: 0.1656
Epoch [14/200], Train Loss: 0.1641, Val Loss: 0.1659
Epoch [15/200], Train Loss: 0.1650, Val Loss: 0.1637
Epoch [16/200], Train Loss: 0.1586, Val Loss: 0.1632
Epoch [17/200], Train Loss: 0.1658, Val Loss: 0.1632
Epoch [18/200], Train Loss: 0.1612, Val Loss: 0.1647
Epoch [19/200], Train Loss: 0.1675, Val Loss: 0.1640
Epoch [20/200], Train Loss: 0.1606, Val Loss: 0.1638
Epoch [21/200], Train Loss: 0.1607, Val Loss: 0.1632
Epoch [22/200], Train Loss: 0.1633, Val Loss: 0.1654
Epoch [23/200], Train Loss: 0.1637, Val Loss: 0.1633
Epoch [24/200], Train Loss: 0.1602, Val Loss: 0.1640
Epoch [25/200], Train Loss: 0.1610, Val Loss: 0.1649
Epoch [26/200], Train Loss: 0.1648, Val Loss: 0.1650
Epoch [27/200], Train Loss: 0.1622, Val Loss: 0.1647
Epoch [28/200], Train Loss: 0.1609, Val Loss: 0.1636
Epoch [29/200], Train Loss: 0.1611, Val Loss: 0.1633
Epoch [30/200], Train Loss: 0.1604, Val Loss: 0.1638
Epoch [31/200], Train Loss: 0.1633, Val Loss: 0.1634
Epoch [32/200], Train Loss: 0.1628, Val Loss: 0.1635
Epoch [33/200], Train Loss: 0.1627, Val Loss: 0.1634
Epoch [34/200], Train Loss: 0.1646, Val Loss: 0.1653
Epoch [35/200], Train Loss: 0.1627, Val Loss: 0.1636
Epoch [36/200], Train Loss: 0.1645, Val Loss: 0.1633
Early stopping
